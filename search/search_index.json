{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"This is a documentation for the data lake project of DSA program. Compiled by Chit","title":"Home"},{"location":"ambari/","text":"","title":"Ambari"},{"location":"deploy/","text":"Prerequisites Hostnames Binding to IPs SSH setup to allow master and slaves communiate without authentication JAVA 8 Hostname Binding sample Hosts file location on linux /etc/hosts 148.61.31.2 precision-master 148.61.31.8 precision-second 148.61.31.134 thinkcenter 148.61.31.121 optiplex1 148.61.31.142 optiplex2 148.61.31.189 optiplex4 SSH setup ssh-keygen -t rsa - Generating Key on Master Node ssh-copy-id -i $HOME/.ssh/id_rsa.pub uid@slavehostname(or)slaveip - Copy the master node\u2019s ssh key to slave\u2019s authorized keys . Repeat this for the rest of slaves. Login to the individual slave and repeat the process until all the keys are added in permutation -- master-salves, slave-master and slave-slaves. Privisioning with wget -O /etc/apt/sources.list.d/ambari.list public-repo-1.hortonworks.com/ambari/ubuntu18/2.x/updates/2.7.3.0/ambari.list - Adding Ambari repository file to /etc/apt/sources.list.d The repository file may require to be updated depend on ambari version and linux distrubtion of the master node. The current one is for ubuntu 18.04 and ambari version is 2.7.3. apt-key adv --recv-keys --keyserver keyserver.ubuntu.com B9733A7A07513CAD - Add the GPG keys of repo to the server sudo apt install ambari-server - Installing Ambari Server debian-based packages sudo ambari-server setup - run Ambari Server daemon as root . Open localhost:8080 in browser and follow the wizard to provision the cluster: Please refer to Ambari original documentation for the configuration: ambari.apache.org Screenshots from the previous wizard Service can be added after the provising. Advised to choose core services to keep the deployment simple and also make troubleshoot easy if something happened in mid-way. Assigning Masters When choosing node for clients applications, consider the Master node if there is no dedicated edge node for them. Co-locating the client applications with services on master can avoid potential problem in slave node because data nodes/node managers are worker nodes those required to process intensively depend on the task assigned by resource manager.","title":"Deployment"},{"location":"deploy/#prerequisites","text":"Hostnames Binding to IPs SSH setup to allow master and slaves communiate without authentication JAVA 8","title":"Prerequisites"},{"location":"deploy/#hostname-binding-sample","text":"Hosts file location on linux /etc/hosts 148.61.31.2 precision-master 148.61.31.8 precision-second 148.61.31.134 thinkcenter 148.61.31.121 optiplex1 148.61.31.142 optiplex2 148.61.31.189 optiplex4","title":"Hostname Binding sample"},{"location":"deploy/#ssh-setup","text":"ssh-keygen -t rsa - Generating Key on Master Node ssh-copy-id -i $HOME/.ssh/id_rsa.pub uid@slavehostname(or)slaveip - Copy the master node\u2019s ssh key to slave\u2019s authorized keys . Repeat this for the rest of slaves. Login to the individual slave and repeat the process until all the keys are added in permutation -- master-salves, slave-master and slave-slaves.","title":"SSH setup"},{"location":"deploy/#privisioning-with","text":"wget -O /etc/apt/sources.list.d/ambari.list public-repo-1.hortonworks.com/ambari/ubuntu18/2.x/updates/2.7.3.0/ambari.list - Adding Ambari repository file to /etc/apt/sources.list.d The repository file may require to be updated depend on ambari version and linux distrubtion of the master node. The current one is for ubuntu 18.04 and ambari version is 2.7.3. apt-key adv --recv-keys --keyserver keyserver.ubuntu.com B9733A7A07513CAD - Add the GPG keys of repo to the server sudo apt install ambari-server - Installing Ambari Server debian-based packages sudo ambari-server setup - run Ambari Server daemon as root . Open localhost:8080 in browser and follow the wizard to provision the cluster: Please refer to Ambari original documentation for the configuration: ambari.apache.org","title":"Privisioning with"},{"location":"deploy/#screenshots-from-the-previous-wizard","text":"Service can be added after the provising. Advised to choose core services to keep the deployment simple and also make troubleshoot easy if something happened in mid-way.","title":"Screenshots from the previous wizard"},{"location":"deploy/#assigning-masters","text":"When choosing node for clients applications, consider the Master node if there is no dedicated edge node for them. Co-locating the client applications with services on master can avoid potential problem in slave node because data nodes/node managers are worker nodes those required to process intensively depend on the task assigned by resource manager.","title":"Assigning Masters"},{"location":"hdfs/","text":"HDFS consists of two Java daemons: Namenode (Master) Datanode (Slave) The DataNode stores the application data and NameNode stores the filesystem metadata. The application data is split into multiple blocks of a fixed size (128 MB by default, but can be configured according to requirement) and then distributed to multiple nodes in the Hadoop cluster, making adequate data replication according to the replication factor setup in the cluster. This replication makes Hadoop fault-tolerant and reliable. The communication between NameNode and DataNode is through robust TCP-based protocols.","title":"HDFS"},{"location":"hive/","text":"Overview Apache Hive is a data warehouse framework for querying and managing large datasets stored in Hadoop distributed filesystems (HDFS). Hive uses its own dialect of SQL called HQL. However, it supports commonly use SQL statement. The interchangeability between SQL and HQL in this pdf . Architecture Hive stores in data in HDFS and in Table format. Metadata for a Hive table is stored in embedded database (default - Derby). Postgre, Mysql & Oracle are supported as well. Role of Metastore The mapping of tables to their directory locations in HDFS and the columns and their definitions are maintained in the Hive metastore. The metastore is a relational database (somewhat ironic) that is written to and read by the Hive client. The object definitions also include the input and output formats for the files represented by the table objects (for example, CSVInputFormat, and so on) and SerDes (short for Serialization/Deserialization functions), which instruct Hive how to extract records and fields from the files. Configuration Minimum Configuration Properties for the hive-site.xml File Property Description Value hive.metastore.warehouse.dir The HDFS directory in which the Hive table data is stored. hdfs://hdfsnamnode:8020/user/hive/warehouse hive.metastore.uris The URI for clients to access the Hive metastore. thrift://hostname:10000 For more details, refer to the Apache Hive documentation at http://hive.apache.org/ . Using Hive There are two ways to access Hive: Hive Command Line: An interface used to execute HiveQL JDBC (Java DataBase Connectivity)/ODBC (Object DataBase Connectivity) driver: This is to establish connectivity to the data storage Apache Hive is not a suitable candidate for OnLine Transaction Processing (OLTP), rather it is more suited for warehousing capabilities (OLAP--OnLine Analytical Processing). It is, however, capable of handling huge datasets of the scale of petabytes quite easily. Connecting Hive from R Connecting to Hive is not as straightforward as MySql and Postgre. As of writing this documentation, the drivers in odbc package from CRAN only supported Hadoop vendors(Cloudera, Hortonworks, MapR, etc.). The workaround to connect non-vendor oriented infrastructure is using JDBC (Java Database Cpmmectivity) wrapper package for R called \" RJDBC \" and low-level interface to Java VM called \" rJava \". Obtaining the hive-jdbc-standalone.jar (for your specific version) from Maven repository . Code below shows the driver creation and connnecting to Hive using driver: library(rJava) library(RJDBC) cp=c(\"hive-jdbc-3.1.0-standalone.jar\") .jinit(classpath=cp) #adding driver to the Java Classpath to be available in system environment variable drv <- JDBC(\"org.apache.hive.jdbc.HiveDriver\", \"hive-jdbc-3.1.0-standalone.jar\", identifier.quote=\"`\") conn <- dbConnect(drv, \"jdbc:hive2://precision-master:10000/default\", \"hadoop\", \"\") df = dbGetQuery(conn, \"select * from gdelt limit 1000\") # the response data is the standard data frame of R To download the above script in R file link Connecting Hive from Python Similarly the connection from Python to Hive also required a wrapper to use JDBC in Python. To install Python JDBC wrapper , execute the pip command - pip install JayDeBeApi Python code to establish connection to Hive and querying import jaydebeapi #con = jaydebeapi.connect('org.apache.hive.jdbc.HiveDriver','jdbc_url, ['username','password'], \"path_to_driver.jar\",) con = jaydebeapi.connect('org.apache.hive.jdbc.HiveDriver','jdbc:hive2://localhost:10000/default',['hadoop',''], \"/home/winc/hive-jdbc-3.1.0-standalone.jar\",) cur = con.cursor() cur.execute(\"select * from gdelt limit 100\") cur.fetchall() To download the above script in Py Notebook file link Partitioning in Hive The basic idea of partitioning in Hive is to maintain the efficiency of the query as the data grows in a table. By partitioning, the table is divided into sequence of subdirectories based upon one or more conditions that typically would be used in WHERE clauses for the table. The column or variable that used to create partitions is not required to include in table stable. Instead, the column becomes a pseudocolumn and used in partitioned by (column string) statement follows at the end of table variables list as shown in below code. CREATE TABLE weblogs ( ip STRING, time_local STRING, method STRING, uri STRING, protocol STRING, status STRING, bytes_sent STRING, referer STRING, useragent STRING) PARTITIONED BY (date STRING) Loading partition statically to existing partitioned table LOAD DATA LOCAL INPATH 'pathto/file.txt' INTO TABLE weblogs PARTITION (date='3/1/2019'); Dynamic partitioning Instead of loading each partition with single SQL statement as shown above, which will result in writing lot of SQL statements for each value of partition key. Hive supports dynamic partitioning with which we can add any number of partitions with single SQL execution. Although dynamic partition loading does not require for partition keys, it needs an intermediary table. The following HQL statement will provide the idea of how dynamic partitioning works: CREATE TEMPORARY TABLE temp_weblogs ( ip STRING, time_local STRING, method STRING, uri STRING, protocol STRING, status STRING, bytes_sent STRING, referer STRING, useragent STRING, date STRING) ROW FORMAT DELIMITED FIELDS TERMINATED BY ',' LINES TERMINATED BY '\\n' STORED AS TEXTFILE; LOCATION \"/pathtofileonHDFS\" The above code is similar to DDL of traditional SQL. The only extension to it is the Hive specific statements for the delimiters of raw data records, fields, location, and file type. After the raw data is query-able from temporary, the create table statement is with one additional HiveQL STORED AS SEQUENCEFILE to automatically create the partition for each value of the partition key. CREATE TABLE partitioned_weblogs ( ip STRING, time_local STRING, method STRING, uri STRING, protocol STRING, status STRING, bytes_sent STRING, referer STRING, useragent STRING) PARTITIONED BY (date STRING) STORED AS SEQUENCEFILE; At this point, all the requirements for dynamic partitioning are met and ready for the final process. INSERT INTO table partitioned_weblogs PARTITION(date) select ip, time_local, method, uri, protocol, status, bytes_sent, referer, useragent, date) FROM temp_weblogs \"One last caveat for the above statement is to include the partition column(s) as the last column(s) (date) in the insert query\".","title":"Hive"},{"location":"hive/#overview","text":"Apache Hive is a data warehouse framework for querying and managing large datasets stored in Hadoop distributed filesystems (HDFS). Hive uses its own dialect of SQL called HQL. However, it supports commonly use SQL statement. The interchangeability between SQL and HQL in this pdf .","title":"Overview"},{"location":"hive/#architecture","text":"Hive stores in data in HDFS and in Table format. Metadata for a Hive table is stored in embedded database (default - Derby). Postgre, Mysql & Oracle are supported as well.","title":"Architecture"},{"location":"hive/#role-of-metastore","text":"The mapping of tables to their directory locations in HDFS and the columns and their definitions are maintained in the Hive metastore. The metastore is a relational database (somewhat ironic) that is written to and read by the Hive client. The object definitions also include the input and output formats for the files represented by the table objects (for example, CSVInputFormat, and so on) and SerDes (short for Serialization/Deserialization functions), which instruct Hive how to extract records and fields from the files.","title":"Role of Metastore"},{"location":"hive/#configuration","text":"Minimum Configuration Properties for the hive-site.xml File Property Description Value hive.metastore.warehouse.dir The HDFS directory in which the Hive table data is stored. hdfs://hdfsnamnode:8020/user/hive/warehouse hive.metastore.uris The URI for clients to access the Hive metastore. thrift://hostname:10000 For more details, refer to the Apache Hive documentation at http://hive.apache.org/ .","title":"Configuration"},{"location":"hive/#using-hive","text":"There are two ways to access Hive: Hive Command Line: An interface used to execute HiveQL JDBC (Java DataBase Connectivity)/ODBC (Object DataBase Connectivity) driver: This is to establish connectivity to the data storage Apache Hive is not a suitable candidate for OnLine Transaction Processing (OLTP), rather it is more suited for warehousing capabilities (OLAP--OnLine Analytical Processing). It is, however, capable of handling huge datasets of the scale of petabytes quite easily.","title":"Using Hive"},{"location":"hive/#connecting-hive-from-r","text":"Connecting to Hive is not as straightforward as MySql and Postgre. As of writing this documentation, the drivers in odbc package from CRAN only supported Hadoop vendors(Cloudera, Hortonworks, MapR, etc.). The workaround to connect non-vendor oriented infrastructure is using JDBC (Java Database Cpmmectivity) wrapper package for R called \" RJDBC \" and low-level interface to Java VM called \" rJava \". Obtaining the hive-jdbc-standalone.jar (for your specific version) from Maven repository . Code below shows the driver creation and connnecting to Hive using driver: library(rJava) library(RJDBC) cp=c(\"hive-jdbc-3.1.0-standalone.jar\") .jinit(classpath=cp) #adding driver to the Java Classpath to be available in system environment variable drv <- JDBC(\"org.apache.hive.jdbc.HiveDriver\", \"hive-jdbc-3.1.0-standalone.jar\", identifier.quote=\"`\") conn <- dbConnect(drv, \"jdbc:hive2://precision-master:10000/default\", \"hadoop\", \"\") df = dbGetQuery(conn, \"select * from gdelt limit 1000\") # the response data is the standard data frame of R To download the above script in R file link","title":"Connecting Hive from R"},{"location":"hive/#connecting-hive-from-python","text":"Similarly the connection from Python to Hive also required a wrapper to use JDBC in Python. To install Python JDBC wrapper , execute the pip command - pip install JayDeBeApi Python code to establish connection to Hive and querying import jaydebeapi #con = jaydebeapi.connect('org.apache.hive.jdbc.HiveDriver','jdbc_url, ['username','password'], \"path_to_driver.jar\",) con = jaydebeapi.connect('org.apache.hive.jdbc.HiveDriver','jdbc:hive2://localhost:10000/default',['hadoop',''], \"/home/winc/hive-jdbc-3.1.0-standalone.jar\",) cur = con.cursor() cur.execute(\"select * from gdelt limit 100\") cur.fetchall() To download the above script in Py Notebook file link","title":"Connecting Hive from Python"},{"location":"hive/#partitioning-in-hive","text":"The basic idea of partitioning in Hive is to maintain the efficiency of the query as the data grows in a table. By partitioning, the table is divided into sequence of subdirectories based upon one or more conditions that typically would be used in WHERE clauses for the table. The column or variable that used to create partitions is not required to include in table stable. Instead, the column becomes a pseudocolumn and used in partitioned by (column string) statement follows at the end of table variables list as shown in below code. CREATE TABLE weblogs ( ip STRING, time_local STRING, method STRING, uri STRING, protocol STRING, status STRING, bytes_sent STRING, referer STRING, useragent STRING) PARTITIONED BY (date STRING)","title":"Partitioning in Hive"},{"location":"hive/#loading-partition-statically-to-existing-partitioned-table","text":"LOAD DATA LOCAL INPATH 'pathto/file.txt' INTO TABLE weblogs PARTITION (date='3/1/2019');","title":"Loading partition statically to existing partitioned table"},{"location":"hive/#dynamic-partitioning","text":"Instead of loading each partition with single SQL statement as shown above, which will result in writing lot of SQL statements for each value of partition key. Hive supports dynamic partitioning with which we can add any number of partitions with single SQL execution. Although dynamic partition loading does not require for partition keys, it needs an intermediary table. The following HQL statement will provide the idea of how dynamic partitioning works: CREATE TEMPORARY TABLE temp_weblogs ( ip STRING, time_local STRING, method STRING, uri STRING, protocol STRING, status STRING, bytes_sent STRING, referer STRING, useragent STRING, date STRING) ROW FORMAT DELIMITED FIELDS TERMINATED BY ',' LINES TERMINATED BY '\\n' STORED AS TEXTFILE; LOCATION \"/pathtofileonHDFS\" The above code is similar to DDL of traditional SQL. The only extension to it is the Hive specific statements for the delimiters of raw data records, fields, location, and file type. After the raw data is query-able from temporary, the create table statement is with one additional HiveQL STORED AS SEQUENCEFILE to automatically create the partition for each value of the partition key. CREATE TABLE partitioned_weblogs ( ip STRING, time_local STRING, method STRING, uri STRING, protocol STRING, status STRING, bytes_sent STRING, referer STRING, useragent STRING) PARTITIONED BY (date STRING) STORED AS SEQUENCEFILE; At this point, all the requirements for dynamic partitioning are met and ready for the final process. INSERT INTO table partitioned_weblogs PARTITION(date) select ip, time_local, method, uri, protocol, status, bytes_sent, referer, useragent, date) FROM temp_weblogs \"One last caveat for the above statement is to include the partition column(s) as the last column(s) (date) in the insert query\".","title":"Dynamic partitioning"},{"location":"llap/","text":"Live Long And Process (LLAP) Hive has become significantly faster thanks to various features and improvements that were built by the community in recent years, including Tez and Cost-based-optimization. The following were needed to take Hive to the next level: Asynchronous spindle-aware IO Pre-fetching and caching of column chunks Multi-threaded JIT-friendly operator pipelines LLAP configuration in details Component Parameter Conf Section of Hive Rule and comments SliderSize slider_am_container_mb hive-interactive-env =yarn.scheduler.minimum-allocation-mb Tez AM coordinator Size tez.am.resource.memory.mb tez-interactive-site =yarn.scheduler.minimum-allocation-mb Number of Cordinators hive.server2.tez.sessions.per.default.queue Settings Number of Concurrent Queries LLAP support. This will result in spawning equal number of TEZ AM. LLAP DaemonSize hive.llap.daemon.yarn.container.mb hive-interactive-site yarn.scheduler.minimum-allocation-mb <= Daemon Size <= yarn.scheduler.maximu-allocation-mb. Rule of thumb always set it to yarn.scheduler.maximu-allocation-mb. Number of Daemon num_llap_nodes_for_llap_daemons hive-interactive-env Number of LLAP Daemons running Number of Daemons num_llap_nodes_for_llap_daemons hive-interactive-env Number of LLAP Daemons running. This will determine total Cache and executors available to run any query on LLAP ExecutorSize hive.tez.container.size hive-interactive-site 4 \u2013 6 GB is the recommended value. For each executor you need to allocate one VCPU Number of Executor hive.llap.daemon.num.executors Determined by number of \u201cMaximum VCore in YARN\u201d LLAP Daemon configuration in details Component PARAMETER NAME SECTION Rule and comments Maximum YARN container Size yarn.scheduler.maximu-allocation-mb. YARN settings This is the maximum amount of memory a Conatiner can be allocated with.Its Recommended to RUN LLAP daemon as a big Container on a node DaemonSize hive.llap.daemon.yarn.container.mb hive-interactive-site yarn.scheduler.minimum-allocation-mb <= Daemon Size <= yarn.scheduler.maximu-allocation-mb. Rule of thumb always set it to yarn.scheduler.maximu-allocation-mb. Headroom llap_headroom_space hive-interactive-env MIN (5% of DaemonSize or 6 GB). Its off heap But part of LLAP Daemon HeapSize llap_heap_size hive-interactive-env Number of Executor * hive.tez.container.size Cache Size hive.llap.io.memory.size hive-interactive-site DaemonSize - HeapSize \u2013 Headroom. Its off heap but part of LLAP daemon LLAP Queue Size Slider Am Size + Number of Tez Conatiners * hive.tez.container.size + Size of LLAP Daemon * Number of LLAP Daemons For more information in-depth on the components of LLAP, Original Document on HIVE LLAP","title":"Hive LLAP"},{"location":"llap/#live-long-and-process-llap","text":"Hive has become significantly faster thanks to various features and improvements that were built by the community in recent years, including Tez and Cost-based-optimization. The following were needed to take Hive to the next level: Asynchronous spindle-aware IO Pre-fetching and caching of column chunks Multi-threaded JIT-friendly operator pipelines","title":"Live Long And Process (LLAP)"},{"location":"llap/#llap-configuration-in-details","text":"Component Parameter Conf Section of Hive Rule and comments SliderSize slider_am_container_mb hive-interactive-env =yarn.scheduler.minimum-allocation-mb Tez AM coordinator Size tez.am.resource.memory.mb tez-interactive-site =yarn.scheduler.minimum-allocation-mb Number of Cordinators hive.server2.tez.sessions.per.default.queue Settings Number of Concurrent Queries LLAP support. This will result in spawning equal number of TEZ AM. LLAP DaemonSize hive.llap.daemon.yarn.container.mb hive-interactive-site yarn.scheduler.minimum-allocation-mb <= Daemon Size <= yarn.scheduler.maximu-allocation-mb. Rule of thumb always set it to yarn.scheduler.maximu-allocation-mb. Number of Daemon num_llap_nodes_for_llap_daemons hive-interactive-env Number of LLAP Daemons running Number of Daemons num_llap_nodes_for_llap_daemons hive-interactive-env Number of LLAP Daemons running. This will determine total Cache and executors available to run any query on LLAP ExecutorSize hive.tez.container.size hive-interactive-site 4 \u2013 6 GB is the recommended value. For each executor you need to allocate one VCPU Number of Executor hive.llap.daemon.num.executors Determined by number of \u201cMaximum VCore in YARN\u201d","title":"LLAP configuration in details"},{"location":"llap/#llap-daemon-configuration-in-details","text":"Component PARAMETER NAME SECTION Rule and comments Maximum YARN container Size yarn.scheduler.maximu-allocation-mb. YARN settings This is the maximum amount of memory a Conatiner can be allocated with.Its Recommended to RUN LLAP daemon as a big Container on a node DaemonSize hive.llap.daemon.yarn.container.mb hive-interactive-site yarn.scheduler.minimum-allocation-mb <= Daemon Size <= yarn.scheduler.maximu-allocation-mb. Rule of thumb always set it to yarn.scheduler.maximu-allocation-mb. Headroom llap_headroom_space hive-interactive-env MIN (5% of DaemonSize or 6 GB). Its off heap But part of LLAP Daemon HeapSize llap_heap_size hive-interactive-env Number of Executor * hive.tez.container.size Cache Size hive.llap.io.memory.size hive-interactive-site DaemonSize - HeapSize \u2013 Headroom. Its off heap but part of LLAP daemon LLAP Queue Size Slider Am Size + Number of Tez Conatiners * hive.tez.container.size + Size of LLAP Daemon * Number of LLAP Daemons For more information in-depth on the components of LLAP, Original Document on HIVE LLAP","title":"LLAP Daemon configuration in details"},{"location":"nodes/","text":"Following table shows the specifications of each node: Master Dell precision T3500 Processor: Intel\u00ae Xeon(R) CPU W3690 @ 3.47GHz \u00d7 12 Memory: 15.7GB Storage: 512GB Samsung SSD 840 320GB ST3320413AS 320GB ST3320413AS Secondary Dell precision T3500 Processor:Intel\u00ae Xeon(R) CPU W3690 @ 3.47GHz \u00d7 6 Memory: 11.7 GiB Storage: 320GB ST3320413AS 320GB ST3320413AS Slaves: ThinkCenter M83 Processor: Intel\u00ae Core\u2122 i5-4570 CPU @ 3.20GHz \u00d7 4 Memory: 15.6 GiB Storage: 250GB ST250DM000-1BD14 250GB WDC WD2500AAKX-7 Dell Optiplex 790 Processor: Intel\u00ae Core\u2122 i3-2120 CPU @ 3.30GHz \u00d7 4 Memory: 15.5 GiB Storage: 250GB WDC WD2500AAKX-7 250GB ST250DM000-1BD14 Dell Optiplex 790 Processor: Intel\u00ae Core\u2122 i3-2120 CPU @ 3.30GHz \u00d7 4 Memory: 15.5 GiB Storage: 160GB ST3160815AS 160GB ST3160318AS Dell Optiplex 790 Processor: Intel\u00ae Core\u2122 i3-2120 CPU @ 3.30GHz \u00d7 4 Memory: 15.5 GiB Storage: 250GB WDC WD2500AAKX-7 160GB ST3160815AS","title":"Nodes Information"},{"location":"ori/","text":"Commands mkdocs new [dir-name] - Create a new project. mkdocs serve - Start the live-reloading docs server. mkdocs build - Build the documentation site. mkdocs help - Print this help message. Project layout mkdocs.yml # The configuration file. docs/ index.md # The documentation homepage. ... # Other markdown pages, images and other files.","title":"Ori"},{"location":"ori/#commands","text":"mkdocs new [dir-name] - Create a new project. mkdocs serve - Start the live-reloading docs server. mkdocs build - Build the documentation site. mkdocs help - Print this help message.","title":"Commands"},{"location":"ori/#project-layout","text":"mkdocs.yml # The configuration file. docs/ index.md # The documentation homepage. ... # Other markdown pages, images and other files.","title":"Project layout"},{"location":"sqoop/","text":"Overview SQL to Hadoop == SQOOP Sqoop is a tool to transfer data between Hadoop and relational database servers. How it works? Sample commands ./sqoop import --connect jdbc:mysql://localhost:3306/classicmodels --username root --table customers --m 1 -P --delete-target-dir - Import from mysql to hadoop","title":"Sqoop"},{"location":"sqoop/#overview","text":"SQL to Hadoop == SQOOP Sqoop is a tool to transfer data between Hadoop and relational database servers.","title":"Overview"},{"location":"sqoop/#how-it-works","text":"Sample commands ./sqoop import --connect jdbc:mysql://localhost:3306/classicmodels --username root --table customers --m 1 -P --delete-target-dir - Import from mysql to hadoop","title":"How it works?"},{"location":"tez/","text":"Overview A YARN-based data processing application framework which allows for a complex directed-acyclic-graph of tasks for processing data. It is considered as a more flexible and powerful successor of the mapreduce framework. tez Features Backward compatibility to Mapreduce framework Improves the MapReduce paradigm dramatically Ability to scale to petabytes of data Optimal resource management Plan re-configuration at run-time Dynamic physical data flow decisions Tez is client side application and it is very simple and easy to try it out. No deployments needed. Comparison with MapReduce Types of queries MapReduce supports batch oriented queries Apache Tez supports interactive queries Processing Model MapReduce always requires a map phasebefore every reduce phases A single Map phase and we may have multiple reduce phases. Responsiveness Slower due to the access of HDFS after every Map and Reduce phase High due to lesser job splitting and HDFS access Vendors\u2019 SQL solutions Cloudera Hortonworks Pivotal MapR Apache Impala written in C++ memory-intensive solution does not use MapReduce Server Daemons are co-located with DataNodes in a Hadoop cluster Apache Tez lower-latency SQL on Hadoop operations DAG (Directed Acyclic Graph) scheduling framework to execute coarse-grained data MapReduce as an execution engine Apache HAWQ enable federation of queries across HDFS MPP(Massively Parallel Processing) built on top of PostgreSQL full-fledged SQL-on-Hadoop solution memory-centric architecture Apache Drill direct open source implementation of Dremel low-latency query engine capability to use objects created with Hive supports multiple data sources:HDFS, object stores, and block stores, and NoSQL stores such as HBase and MongoDB","title":"Tez"},{"location":"tez/#overview","text":"A YARN-based data processing application framework which allows for a complex directed-acyclic-graph of tasks for processing data. It is considered as a more flexible and powerful successor of the mapreduce framework. tez","title":"Overview"},{"location":"tez/#features","text":"Backward compatibility to Mapreduce framework Improves the MapReduce paradigm dramatically Ability to scale to petabytes of data Optimal resource management Plan re-configuration at run-time Dynamic physical data flow decisions Tez is client side application and it is very simple and easy to try it out. No deployments needed.","title":"Features"},{"location":"tez/#comparison-with-mapreduce","text":"Types of queries MapReduce supports batch oriented queries Apache Tez supports interactive queries Processing Model MapReduce always requires a map phasebefore every reduce phases A single Map phase and we may have multiple reduce phases. Responsiveness Slower due to the access of HDFS after every Map and Reduce phase High due to lesser job splitting and HDFS access","title":"Comparison with MapReduce"},{"location":"tez/#vendors-sql-solutions","text":"Cloudera Hortonworks Pivotal MapR Apache Impala written in C++ memory-intensive solution does not use MapReduce Server Daemons are co-located with DataNodes in a Hadoop cluster Apache Tez lower-latency SQL on Hadoop operations DAG (Directed Acyclic Graph) scheduling framework to execute coarse-grained data MapReduce as an execution engine Apache HAWQ enable federation of queries across HDFS MPP(Massively Parallel Processing) built on top of PostgreSQL full-fledged SQL-on-Hadoop solution memory-centric architecture Apache Drill direct open source implementation of Dremel low-latency query engine capability to use objects created with Hive supports multiple data sources:HDFS, object stores, and block stores, and NoSQL stores such as HBase and MongoDB","title":"Vendors\u2019 SQL solutions"},{"location":"yarn/","text":"Two imporant components in YARN: ResourceManager (Master) NodeManager (Slave) Resource manager utilizes resources in a dynamic fashion by managing cluster resources efficiently and allowing applications to do the job rather than figuring out their impact on the resources. ResourceManager has two sub-components known as: Application Manager Scheduler for monitoring applications Each node has one NodeManager and resources within a node are allocated and handled. It's considered as the Slave node of YARN. It accepts requests from Resource Manager and reports back on the health and resources of the nodes. The following figure shows YARN\u2019s architecture and its components in detail along with how it works:","title":"YARN"}]}